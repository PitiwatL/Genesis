{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2132f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pitiw\\miniconda3\\envs\\open-oasis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "# Create a 4D tensor of shape (batch, channels, height, width)\n",
    "tensor = torch.rand(10, 3, 32, 32)  # Example: a batch of 10 RGB images 32x32\n",
    "# Rearrange to (batch, height, width, channels) for image processing libraries that expect this format\n",
    "rearranged = rearrange(tensor, 'b c h w -> b h w c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d44c2",
   "metadata": {},
   "source": [
    "Reduce the tensor's channel dimension by taking the mean, resulting in a grayscale image \\\n",
    "Since the \"c\" is vanished so it does mean there's agrregation there. \\\n",
    "It averages out the channel column. So, if there're red green blue columns, it averages all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e803f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce\n",
    "\n",
    "# Reduce methods 'min', 'max', 'sum', 'mean', 'prod', 'any', 'all'\n",
    "grayscale = reduce(tensor, 'b c h w -> b h w', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83131b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "# Repeat each image in the batch 4 times along a new dimension\n",
    "# It duplicated the batch dimension\n",
    "repeated = repeat(tensor, 'b c h w -> (repeat b) c h w', repeat=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbf345",
   "metadata": {},
   "source": [
    "Spliting and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 32, 32])\n",
      "torch.Size([3, 10, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Act like (c rgb) a singlw column but later want to separate c away as a separate vec dimension\n",
    "# Not just act only but we can separate them\n",
    "print(tensor.shape)\n",
    "x = rearrange(tensor, 'b (c rgb) h w -> rgb b c h w', rgb=3)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5e67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split channels\n",
    "red, green, blue = rearrange(tensor, 'b (c rgb) h w -> rgb b c h w', rgb=3)\n",
    "\n",
    "# Example processing (identity here)\n",
    "processed_red, processed_green, processed_blue = red, green, blue\n",
    "# Merge channels back\n",
    "merged = rearrange([processed_red, processed_green, processed_blue], 'rgb b c h w -> b (rgb c) h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304fcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten spatial dimensions\n",
    "flattened = rearrange(tensor, 'b c h w -> b (c h w)')\n",
    "\n",
    "# Example neural network operation\n",
    "# output = model(flattened)\n",
    "# Unflatten back to spatial dimensions (assuming output has shape b, features)\n",
    "# unflattened = rearrange(output, 'b (c h w) -> b c h w', c=3, h=32, w=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed818b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3072])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255530c",
   "metadata": {},
   "source": [
    "## Attention without eniops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99af34b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "def simplified_self_attention(q, k, v):\n",
    "    \"\"\"\n",
    "    A simplified self-attention mechanism.\n",
    "    Args:\n",
    "        q, k, v (torch.Tensor): Queries, Keys, and Values. Shape: [batch_size, num_tokens, feature_dim]\n",
    "    Returns:\n",
    "        torch.Tensor: The result of the attention mechanism.\n",
    "    \"\"\"\n",
    "    # Compute the dot product between queries and keys\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Multiply by values\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    return output\n",
    "\n",
    "# Example tensors representing queries, keys, and values\n",
    "batch_size, num_tokens, feature_dim = 10, 16, 64\n",
    "q = torch.rand(batch_size, num_tokens, feature_dim)\n",
    "k = torch.rand(batch_size, num_tokens, feature_dim)\n",
    "v = torch.rand(batch_size, num_tokens, feature_dim)\n",
    "# Apply self-attention\n",
    "attention_output = simplified_self_attention(q, k, v)\n",
    "print(\"Output shape:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in (q, k, v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_self_attention(q, k, v, num_heads=8):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention using Einops for splitting and merging heads.\n",
    "    \"\"\"\n",
    "    batch_size, num_tokens, feature_dim = q.shape\n",
    "    head_dim = feature_dim // num_heads\n",
    "    \n",
    "    # Split into multiple heads\n",
    "    q, k, v = [\n",
    "        rearrange(x, 'b t (h d) -> b h t d', h=num_heads)\n",
    "        for x in (q, k, v)\n",
    "    ]\n",
    "    \n",
    "    # Apply self-attention to each head\n",
    "    output = simplified_self_attention(q, k, v)\n",
    "    \n",
    "    # Merge the heads back\n",
    "    output = rearrange(output, 'b h t d -> b t (h d)')\n",
    "    return output\n",
    "\n",
    "# Apply multi-head self-attention\n",
    "multi_head_attention_output = multi_head_self_attention(q, k, v)\n",
    "print(\"Multi-head output shape:\", multi_head_attention_output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
